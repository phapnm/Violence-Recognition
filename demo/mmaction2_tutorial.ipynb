{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VcjSRFELVbNk"
   },
   "source": [
    "# MMAction2 Tutorial\n",
    "\n",
    "Welcome to MMAction2! This is the official colab tutorial for using MMAction2. In this tutorial, you will learn\n",
    "- Perform inference with a MMAction2 recognizer.\n",
    "- Train a new recognizer with a new dataset.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LqHGkGEVqpm"
   },
   "source": [
    "## Install MMAction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "Bf8PpPXtVvmg",
    "outputId": "82418d99-3c91-49c6-db02-48193d41e9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5PAJ4ArzV5Ry",
    "outputId": "ebd1f340-db77-4ef1-bd34-c53392fc25eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already up-to-date: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
      "Requirement already up-to-date: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
      "Cloning into 'mmaction2'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 4740 (delta 1), reused 4 (delta 1), pack-reused 4732\u001b[K\n",
      "Receiving objects: 100% (4740/4740), 29.67 MiB | 16.12 MiB/s, done.\n",
      "Resolving deltas: 100% (3174/3174), done.\n",
      "/content/mmaction2\n",
      "Obtaining file:///content/mmaction2\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mmaction==0.1.0+78b68d5) (3.2.2)\n",
      "Collecting mmcv\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/13/ffb0e134ab220901806299e4da010d5b2f6a96c3d3ed8a8705c813dfbf56/mmcv-1.0.2.tar.gz (234kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 7.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mmaction==0.1.0+78b68d5) (1.18.5)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.6/dist-packages (from mmaction==0.1.0+78b68d5) (4.1.2.30)\n",
      "Collecting Pillow<=6.2.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/bbbc569f98f47813c50a116b539d97b3b17a86ac7a309f83b2022d26caf2/Pillow-6.2.2-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 19.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmaction==0.1.0+78b68d5) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmaction==0.1.0+78b68d5) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmaction==0.1.0+78b68d5) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmaction==0.1.0+78b68d5) (2.4.7)\n",
      "Collecting addict\n",
      "  Downloading https://files.pythonhosted.org/packages/14/6f/beb258220417c1a0fe11e842f2e012a1be7eeeaa72a1d10ba17a804da367/addict-2.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mmcv->mmaction==0.1.0+78b68d5) (3.13)\n",
      "Collecting yapf\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/5d/d84677fe852bc5e091739acda444a9b6700ffc6b11a21b00dd244c8caef0/yapf-0.30.0-py2.py3-none-any.whl (190kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 43.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.6/dist-packages (from mmcv->mmaction==0.1.0+78b68d5) (4.1.2.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->mmaction==0.1.0+78b68d5) (1.12.0)\n",
      "Building wheels for collected packages: mmcv\n",
      "  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mmcv: filename=mmcv-1.0.2-cp36-cp36m-linux_x86_64.whl size=383274 sha256=36ddb4265db0e2db06c32c013a24ab4c3143048554bba6665abf58a245462209\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/72/2f/090938947dcf1975a513b99748fcaf0277d18e834cf5d9095a\n",
      "Successfully built mmcv\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: addict, yapf, mmcv, Pillow, mmaction\n",
      "  Found existing installation: Pillow 7.0.0\n",
      "    Uninstalling Pillow-7.0.0:\n",
      "      Successfully uninstalled Pillow-7.0.0\n",
      "  Running setup.py develop for mmaction\n",
      "Successfully installed Pillow-6.2.2 addict-2.2.1 mmaction mmcv-1.0.2 yapf-0.30.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting decord\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/2f/8578948cfdb9662a718ffb817291fc135084006f9558569a5af05fec3908/decord-0.4.0-py2.py3-none-manylinux1_x86_64.whl (11.8MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8MB 25kB/s \n",
      "\u001b[?25hCollecting PyTurboJPEG\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/f5/6106c673096b1bc1af716bbc9b17e542c77ccad5fafd150afb91ff18a6e8/PyTurboJPEG-1.4.1.tar.gz\n",
      "Collecting av\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/62/9a992be76f8e13ce0e3a24a838191b546805545116f9fc869bd11bd21b5f/av-8.0.2-cp36-cp36m-manylinux2010_x86_64.whl (36.9MB)\n",
      "\u001b[K     |████████████████████████████████| 36.9MB 68kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from decord->-r requirements/optional.txt (line 1)) (1.18.5)\n",
      "Building wheels for collected packages: PyTurboJPEG\n",
      "  Building wheel for PyTurboJPEG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyTurboJPEG: filename=PyTurboJPEG-1.4.1-cp36-none-any.whl size=7002 sha256=067e8a7fad1e7df3380d2245087766cc79f8bb38cf98d93f24ab1bea83e0bd9c\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/21/97/152eed6e60d59f1c432139dee7a2e89de44026ef1855b4c4d7\n",
      "Successfully built PyTurboJPEG\n",
      "Installing collected packages: decord, PyTurboJPEG, av\n",
      "Successfully installed PyTurboJPEG-1.4.1 av-8.0.2 decord-0.4.0\n"
     ]
    }
   ],
   "source": [
    "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
    "!pip install -U torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Install mmaction2\n",
    "!rm -rf mmaction2\n",
    "!git clone https://github.com/open-mmlab/mmaction2.git\n",
    "%cd mmaction2\n",
    "!pip install -e .\n",
    "!pip install -r requirements/optional.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "No_zZAFpWC-a",
    "outputId": "39e61b80-65d6-4d85-8a5c-d58db2b34681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1+cu101 True\n",
      "0.1.0+78b68d5\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXf7oV5DWdab"
   },
   "source": [
    "## Perform inference with a MMAction2 recognizer\n",
    "MMAction2 already provides high level APIs to do inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "64CW6d_AaT-Q",
    "outputId": "5bdec554-ff27-4c93-975f-b747a93e80bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-12 07:19:39--  https://openmmlab.oss-accelerate.aliyuncs.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
      "Resolving openmmlab.oss-accelerate.aliyuncs.com (openmmlab.oss-accelerate.aliyuncs.com)... 47.254.186.154\n",
      "Connecting to openmmlab.oss-accelerate.aliyuncs.com (openmmlab.oss-accelerate.aliyuncs.com)|47.254.186.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97579339 (93M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’\n",
      "\n",
      "checkpoints/tsn_r50 100%[===================>]  93.06M  11.4MB/s    in 8.1s    \n",
      "\n",
      "2020-07-12 07:19:48 (11.4 MB/s) - ‘checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’ saved [97579339/97579339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://openmmlab.oss-accelerate.aliyuncs.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth \\\n",
    "      -O checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNZB7NoSabzj"
   },
   "outputs": [],
   "source": [
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "\n",
    "# Choose to use a config and initialize the recognizer\n",
    "config = 'configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "# Initialize the recognizer\n",
    "model = init_recognizer(config, checkpoint, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEMsBnpHapAn"
   },
   "outputs": [],
   "source": [
    "# Use the recognizer to do inference\n",
    "video = 'demo/demo.mp4'\n",
    "label = 'demo/label_map.txt'\n",
    "results = inference_recognizer(model, video, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "NIyJXqfWathq",
    "outputId": "79971aa6-cee8-425a-a58f-86159779792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm wrestling:  29.61644\n",
      "rock scissors paper:  10.754843\n",
      "shaking hands:  9.908401\n",
      "clapping:  9.189913\n",
      "massaging feet:  8.305306\n"
     ]
    }
   ],
   "source": [
    "# Let's show the results\n",
    "for result in results:\n",
    "    print(f'{result[0]}: ', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QuZG8kZ2fJ5d"
   },
   "source": [
    "## Train a recognizer on customized dataset\n",
    "\n",
    "To train a new recognizer, there are usually three things to do:\n",
    "1. Support a new dataset\n",
    "2. Modify the config\n",
    "3. Train a new recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neEFyxChfgiJ"
   },
   "source": [
    "### Support a new dataset\n",
    "\n",
    "In this tutorial, we gives an example to convert the data into the format of existing datasets. Other methods and more advanced usages can be found in the [doc](/docs/tutorials/new_dataset.md)\n",
    "\n",
    "Firstly, let's download a tiny dataset obtained from [Kinetics-400](https://deepmind.com/research/open-source/open-source-datasets/kinetics/). We select 30 videos with their labels as train dataset and 10 videos with their labels as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "gjsUj9JzgUlJ",
    "outputId": "cdf115bc-e8ed-440f-c779-2cf5af565556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'kinetics400_tiny.zip*': No such file or directory\n",
      "--2020-07-12 07:20:41--  https://openmmlab.oss-accelerate.aliyuncs.com/mmaction/kinetics400_tiny.zip\n",
      "Resolving openmmlab.oss-accelerate.aliyuncs.com (openmmlab.oss-accelerate.aliyuncs.com)... 47.254.186.154\n",
      "Connecting to openmmlab.oss-accelerate.aliyuncs.com (openmmlab.oss-accelerate.aliyuncs.com)|47.254.186.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18308682 (17M) [application/zip]\n",
      "Saving to: ‘kinetics400_tiny.zip’\n",
      "\n",
      "kinetics400_tiny.zi 100%[===================>]  17.46M  10.0MB/s    in 1.7s    \n",
      "\n",
      "2020-07-12 07:20:44 (10.0 MB/s) - ‘kinetics400_tiny.zip’ saved [18308682/18308682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download, decompress the data\n",
    "!rm kinetics400_tiny.zip*\n",
    "!rm -rf kinetics400_tiny\n",
    "!wget https://openmmlab.oss-accelerate.aliyuncs.com/mmaction/kinetics400_tiny.zip\n",
    "!unzip kinetics400_tiny.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AbZ-o7V6hNw4",
    "outputId": "08d42431-5971-47ca-cd01-91f5313a6dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-440\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
      "Need to get 40.7 kB of archives.\n",
      "After this operation, 105 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
      "Fetched 40.7 kB in 0s (885 kB/s)\n",
      "Selecting previously unselected package tree.\n",
      "(Reading database ... 144379 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
      "Unpacking tree (1.7.0-5) ...\n",
      "Setting up tree (1.7.0-5) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "kinetics400_tiny\n",
      "├── kinetics_tiny_train_video.txt\n",
      "├── kinetics_tiny_val_video.txt\n",
      "├── train\n",
      "│   ├── 27_CSXByd3s.mp4\n",
      "│   ├── 34XczvTaRiI.mp4\n",
      "│   ├── A-wiliK50Zw.mp4\n",
      "│   ├── D32_1gwq35E.mp4\n",
      "│   ├── D92m0HsHjcQ.mp4\n",
      "│   ├── DbX8mPslRXg.mp4\n",
      "│   ├── FMlSTTpN3VY.mp4\n",
      "│   ├── h10B9SVE-nk.mp4\n",
      "│   ├── h2YqqUhnR34.mp4\n",
      "│   ├── iRuyZSKhHRg.mp4\n",
      "│   ├── IyfILH9lBRo.mp4\n",
      "│   ├── kFC3KY2bOP8.mp4\n",
      "│   ├── LvcFDgCAXQs.mp4\n",
      "│   ├── O46YA8tI530.mp4\n",
      "│   ├── oMrZaozOvdQ.mp4\n",
      "│   ├── oXy-e_P_cAI.mp4\n",
      "│   ├── P5M-hAts7MQ.mp4\n",
      "│   ├── phDqGd0NKoo.mp4\n",
      "│   ├── PnOe3GZRVX8.mp4\n",
      "│   ├── R8HXQkdgKWA.mp4\n",
      "│   ├── RqnKtCEoEcA.mp4\n",
      "│   ├── soEcZZsBmDs.mp4\n",
      "│   ├── TkkZPZHbAKA.mp4\n",
      "│   ├── T_TMNGzVrDk.mp4\n",
      "│   ├── WaS0qwP46Us.mp4\n",
      "│   ├── Wh_YPQdH1Zg.mp4\n",
      "│   ├── WWP5HZJsg-o.mp4\n",
      "│   ├── xGY2dP0YUjA.mp4\n",
      "│   ├── yLC9CtWU5ws.mp4\n",
      "│   └── ZQV4U2KQ370.mp4\n",
      "└── val\n",
      "    ├── 0pVGiAU6XEA.mp4\n",
      "    ├── AQrbRSnRt8M.mp4\n",
      "    ├── b6Q_b7vgc7Q.mp4\n",
      "    ├── ddvJ6-faICE.mp4\n",
      "    ├── IcLztCtvhb8.mp4\n",
      "    ├── ik4BW3-SCts.mp4\n",
      "    ├── jqRrH30V0k4.mp4\n",
      "    ├── SU_x2LQqSLs.mp4\n",
      "    ├── u4Rm6srmIS8.mp4\n",
      "    └── y5Iu7XkTqV0.mp4\n",
      "\n",
      "2 directories, 42 files\n"
     ]
    }
   ],
   "source": [
    "# Check the directory structure of the tiny data\n",
    "\n",
    "# Install tree first\n",
    "!apt-get -q install tree\n",
    "!tree kinetics400_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "fTdi6dI0hY3g",
    "outputId": "6c26f345-2530-4d69-f0e4-075337888b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D32_1gwq35E.mp4 0\n",
      "iRuyZSKhHRg.mp4 1\n",
      "oXy-e_P_cAI.mp4 0\n",
      "34XczvTaRiI.mp4 1\n",
      "h2YqqUhnR34.mp4 0\n",
      "O46YA8tI530.mp4 0\n",
      "kFC3KY2bOP8.mp4 1\n",
      "WWP5HZJsg-o.mp4 1\n",
      "phDqGd0NKoo.mp4 1\n",
      "yLC9CtWU5ws.mp4 0\n",
      "27_CSXByd3s.mp4 1\n",
      "IyfILH9lBRo.mp4 1\n",
      "T_TMNGzVrDk.mp4 1\n",
      "TkkZPZHbAKA.mp4 0\n",
      "PnOe3GZRVX8.mp4 1\n",
      "soEcZZsBmDs.mp4 1\n",
      "FMlSTTpN3VY.mp4 1\n",
      "WaS0qwP46Us.mp4 0\n",
      "A-wiliK50Zw.mp4 1\n",
      "oMrZaozOvdQ.mp4 1\n",
      "ZQV4U2KQ370.mp4 0\n",
      "DbX8mPslRXg.mp4 1\n",
      "h10B9SVE-nk.mp4 1\n",
      "P5M-hAts7MQ.mp4 0\n",
      "R8HXQkdgKWA.mp4 0\n",
      "D92m0HsHjcQ.mp4 0\n",
      "RqnKtCEoEcA.mp4 0\n",
      "LvcFDgCAXQs.mp4 0\n",
      "xGY2dP0YUjA.mp4 0\n",
      "Wh_YPQdH1Zg.mp4 0\n"
     ]
    }
   ],
   "source": [
    "# After downloading the data, we need to check the annotation format\n",
    "!cat kinetics400_tiny/kinetics_tiny_train_video.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bq0mxmEi29H"
   },
   "source": [
    "According to the format defined in [`VideoDataset`](./datasets/video_dataset.py), each line indicates a sample video with the filepath and label, which are split with a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ht_DGJA9jQar"
   },
   "source": [
    "### Modify the config\n",
    "\n",
    "In the next step, we need to modify the config for the training.\n",
    "To accelerate the process, we finetune a recognizer using a pre-trained recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjCcmCKOjktc"
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tc8YhFFGjp3e"
   },
   "source": [
    "Given a config that trains a TSN model on kinetics400-full dataset, we need to modify some values to use it for training TSN on Kinetics400-tiny dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlhu9byjjt-K"
   },
   "outputs": [],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'kinetics400_tiny/train/'\n",
    "cfg.data_root_val = 'kinetics400_tiny/val/'\n",
    "cfg.ann_file_train = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
    "cfg.ann_file_val = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.ann_file_test = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.data.test.data_prefix = 'kinetics400_tiny/val/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'kinetics400_tiny/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'kinetics400_tiny/val/'\n",
    "\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = cfg.data.videos_per_gpu // 16\n",
    "cfg.optimizer.lr = cfg.optimizer.lr / 8 / 16\n",
    "cfg.total_epochs = 30\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 10\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 5\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tES-qnZ3k38Z"
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "21a4bef0124d48fd850fc768766da9b5",
      "810d89282d9d4c939ab4d3cb904c3954",
      "ca6d1ea713f04397ad424fed870e71b2",
      "ca1b92209d9a4109b0d80330005762cf",
      "5c9a171c5b6249e994147896ee1512e7",
      "92c0bea08845438299ab62d4ec7b8c42",
      "9f323e47627a4bdebbf33ae24349b62d",
      "3ded7bdc0e2c4ad890b8a1d9261649c9"
     ]
    },
    "colab_type": "code",
    "id": "dDBWkdDRk6oz",
    "outputId": "d02f1f5d-28bf-4818-f4b2-f16e72c6a9dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/run/anaconda3/envs/mmaction/lib/python3.7/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e322cf97d082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Build the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Build the recognizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdSd7oTLlxIf"
   },
   "source": [
    "### Understand the log\n",
    "From the log, we can have a basic understanding the training process and know how well the recognizer is trained.\n",
    "\n",
    "Firstly, the ResNet-50 backbone pre-trained on ImageNet is loaded, this is a common practice since training from scratch is more cost. The log shows that all the weights of the ResNet-50 backbone are loaded except the `fc.bias` and `fc.weight`.\n",
    "\n",
    "Second, since the dataset we are using is small, we loaded a TSN model and finetune it for action recognition.\n",
    "The original TSN is trained on original Kinetics-400 dataset which contains 400 classes but Kinetics-400 Tiny dataset only have 2 classes. Therefore, the last FC layer of the pre-trained TSN for classification has different weight shape and is not used.\n",
    "\n",
    "Third, after training, the recognizer is evaluated by the default evaluation. The results show that the recognizer achieves 100% top1 accuracy and 100% top5 accuracy on the val dataset,\n",
    " \n",
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryVoSfZVmogw"
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "eyY3hCMwyTct",
    "outputId": "4aedc0b1-f75f-4410-beae-cc60f50ab930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 0.5 task/s, elapsed: 18s, ETA:     0s\n",
      "Evaluating top_k_accuracy...\n",
      "\n",
      "top1_acc\t1.0000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy...\n",
      "\n",
      "mean_acc\t1.0000\n",
      "top1_acc: 1.0000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM824ReP9DEsaabcv73pDaG",
   "include_colab_link": true,
   "name": "Copy of MMAction2 Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mmaction",
   "language": "python",
   "name": "mmaction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21a4bef0124d48fd850fc768766da9b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca6d1ea713f04397ad424fed870e71b2",
       "IPY_MODEL_ca1b92209d9a4109b0d80330005762cf"
      ],
      "layout": "IPY_MODEL_810d89282d9d4c939ab4d3cb904c3954"
     }
    },
    "3ded7bdc0e2c4ad890b8a1d9261649c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c9a171c5b6249e994147896ee1512e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "810d89282d9d4c939ab4d3cb904c3954": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92c0bea08845438299ab62d4ec7b8c42": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f323e47627a4bdebbf33ae24349b62d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca1b92209d9a4109b0d80330005762cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ded7bdc0e2c4ad890b8a1d9261649c9",
      "placeholder": "​",
      "style": "IPY_MODEL_9f323e47627a4bdebbf33ae24349b62d",
      "value": " 97.8M/97.8M [00:26&lt;00:00, 3.87MB/s]"
     }
    },
    "ca6d1ea713f04397ad424fed870e71b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92c0bea08845438299ab62d4ec7b8c42",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c9a171c5b6249e994147896ee1512e7",
      "value": 102502400
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
